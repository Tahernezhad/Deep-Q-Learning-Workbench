# Deep Qâ€‘Learning Workbench (PyTorch)

A clean, modular Deep Qâ€‘Learning (DQN) implementation in PyTorch with support for **Double DQN**, **Dueling networks**, and multiple backbones (MLP, 1Dâ€‘CNN, LSTM). The repo includes a training script, a replay script (to watch saved policies), video recording, and simple result tracking â€” designed to be easy to extend to new Gymnasium environments.

## ğŸ¥ Output

<p align="center">
  <img src="./assets/cart_pole.gif" width="360" alt="CartPole-v1 demo">
  &nbsp;&nbsp;&nbsp;
  <img src="./assets/lunar_lander.gif" width="360" alt="LunarLander-v3 demo">
</p>

---

## âœ¨ Features

* **Algorithms**: DQN, Double DQN, Dueling architecture (toggle in config)
* **Backbones**: MLP, 1Dâ€‘CNN, LSTM (select via `MODEL_TYPE`)
* **Targets**: hard copy or **soft/Polyak** updates
* **Loss**: Huber (default) or MSE
* **Replay buffer**: uniform experience replay
* **Logging/Artifacts**: movingâ€‘average score, reward plot, best checkpoint
* **Video**: optional training videos; replay videos on demand
* **Reproducibility**: global seed toggle (note: full determinism is environmentâ€‘dependent)

---

## ğŸ“¦ Environment setup

Use the provided Conda file:

```bash
conda env create -f environment.yml
conda activate rl
```

> **Note:** If you donâ€™t have a GPU, use the CPUâ€‘only variant of the environment or remove the CUDA line from `environment.yml`.

---

## ğŸ—‚ï¸ Project structure

```
Deep-Q-Learning-Workbench/
â”œâ”€â”€ config.py            # Hyperparameters & switches (env, model, loss, target update, etc.)
â”œâ”€â”€ dqn_agent.py         # DQN agent (policy/target nets, action select, optimize step)
â”œâ”€â”€ networks.py          # MLP / CNN1D / LSTM (+ dueling variants)
â”œâ”€â”€ replay_buffer.py     # Uniform replay buffer
â”œâ”€â”€ utils.py             # Seeding, plotting, checkpoint & config save
â”œâ”€â”€ main.py              # Training entrypoint
â”œâ”€â”€ replay.py            # Load a saved model and replay episodes (optional video)
â”œâ”€â”€ environment.yml      # Conda environment spec
â””â”€â”€ results/             # Autoâ€‘created run folders (checkpoints, plots, videos)
```

Each training run creates a timestamped folder under `results/`:

```
results/<ENV_NAME>_YYYYmmdd_HHMMSS/
â”œâ”€â”€ best_model.pth
â”œâ”€â”€ hyperparameters.txt
â”œâ”€â”€ reward_plot.png
â”œâ”€â”€ total_rewards.txt
â””â”€â”€ videos/ (optional, if enabled)
```

---

## âš™ï¸ Configuration

All switches live in **`config.py`**. Common ones:

* **Environment**

  * `ENV_NAME`: `'CartPole-v1'` or `'LunarLander-v3'` (extend with any Gymnasium env)
  * `SEED`: random seed
* **Model**

  * `MODEL_TYPE`: `'MLP' | 'CNN1D' | 'LSTM'`
  * `dueling_network`: `True|False`
* **Algorithm**

  * `double_dqn`: `True|False`
  * `LOSS`: `'huber' | 'mse'`
  * `SOFT_UPDATE`: `True|False`; `TAU`: Polyak coefficient
  * `GAMMA`: discount factor
* **Optimization**

  * `LEARNING_RATE`, `BATCH_SIZE`, `REPLAY_BUFFER_SIZE`, `WARMUP_STEPS`
  * `TARGET_UPDATE_FREQ` (used if `SOFT_UPDATE=False`)
* **Exploration**

  * `EPSILON_START`, `EPSILON_END`, `EPSILON_DECAY`
* **Training**

  * `NUM_EPISODES`, `MAX_STEPS_PER_EPISODE`, `MOVING_AVG_WINDOW`, `REPORT_INTERVAL`
* **I/O**

  * `SAVE_MODEL`: save best checkpoint by movingâ€‘average score
  * `RECORD_VIDEO`: record training videos every `VIDEO_RECORD_INTERVAL`

---

## ğŸš€ Train

1. Pick the environment and model in `config.py`.
2. Run training:

```bash
python main.py
```

Artifacts are written to `results/<ENV_NAME>_<timestamp>/`.

> Tip: enable/disable training video recording via `RECORD_VIDEO` in `config.py`.

---

## ğŸ¬ Replay a saved model

Use **`replay.py`** to watch the policy or record MP4s.

**Option A â€” edit the constants at the top of `replay.py`:**

* Set `RESULTS_PATH` to a specific run folder, e.g. `results/LunarLander-v3_20250914_220000`
* Toggle `SAVE_VIDEO=True` to write MP4s to `replay_videos/` inside that run

```bash
python replay.py
```

**Option B â€” (if your version supports CLI flags)**

```bash
python replay.py \
  --checkpoint results/LunarLander-v3_20250914_220000/best_model.pth \
  --episodes 10 --epsilon 0.0 --record-dir replay_videos
```

> If you replay on a different machine (CPU vs GPU), loading still works thanks to mapâ€‘location logic.

---

## ğŸ” Notes & tips

* **Box2D**: `LunarLander-v3` requires Box2D. If pip gives trouble, the Conda package `box2d-py` is included in the environment.
* **Video encoding**: Recording uses Gymnasiumâ€™s `RecordVideo`; if FFmpeg is missing, install `imageio-ffmpeg` (included in the env) or your systemâ€™s `ffmpeg`.
* **Determinism**: RL training is inherently stochastic. Seeding improves repeatability but slight differences are expected.
* **Extending to new envs**: switch `ENV_NAME`, adjust `n_states`/`n_actions` if using custom spaces, and youâ€™re good to go.

---

## ğŸ§ª Roadmap

* Prioritized Experience Replay
* NoisyNets or parameterâ€‘space noise for exploration
* Frame stacking / sequence sampling for the LSTM path

---

## ğŸ™Œ Acknowledgements

* [Gymnasium](https://github.com/Farama-Foundation/Gymnasium)
* [PyTorch](https://pytorch.org/)


